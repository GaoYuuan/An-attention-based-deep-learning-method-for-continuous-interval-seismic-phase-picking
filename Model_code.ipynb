{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c75cf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset,DataLoader,Subset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "from sklearn.metrics import classification_report, confusion_matrix,precision_score, recall_score, f1_score\n",
    "import seaborn as sns\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97475cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, channels, ratio=4):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.ratio = ratio\n",
    "        \n",
    "        # 全局平均池化和最大池化\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool1d(1)\n",
    "        \n",
    "        # 共享的MLP\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(channels, channels // ratio),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channels // ratio, channels)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, channels, sequence_length)\n",
    "        batch_size, channels, seq_len = x.size()\n",
    "        \n",
    "        # 全局平均池化和最大池化\n",
    "        avg_out = self.avg_pool(x).view(batch_size, channels)  # (B, C)\n",
    "        max_out = self.max_pool(x).view(batch_size, channels)  # (B, C)\n",
    "        \n",
    "        # 通过MLP\n",
    "        avg_out = self.mlp(avg_out)\n",
    "        max_out = self.mlp(max_out)\n",
    "        \n",
    "        # 相加并应用sigmoid\n",
    "        out = torch.sigmoid(avg_out + max_out)\n",
    "        out = out.view(batch_size, channels, 1)  # (B, C, 1)\n",
    "        \n",
    "        # 与输入相乘\n",
    "        return x * out\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self.conv = nn.Conv1d(2, 1, kernel_size=1, padding=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, channels, sequence_length)\n",
    "        \n",
    "        # 计算平均值和最大值\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)  # (B, 1, L)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)  # (B, 1, L)\n",
    "        \n",
    "        # 拼接\n",
    "        concat = torch.cat([avg_out, max_out], dim=1)  # (B, 2, L)\n",
    "        \n",
    "        # 1D卷积\n",
    "        out = self.conv(concat)  # (B, 1, L)\n",
    "        out = torch.sigmoid(out)\n",
    "        \n",
    "        # 与输入相乘\n",
    "        return x * out\n",
    "\n",
    "class CBAMBlock(nn.Module):\n",
    "    def __init__(self, channels, ratio=4):\n",
    "        super(CBAMBlock, self).__init__()\n",
    "        self.channel_attention = ChannelAttention(channels, ratio)\n",
    "        self.spatial_attention = SpatialAttention()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.channel_attention(x)\n",
    "        x = self.spatial_attention(x)\n",
    "        return x\n",
    "\n",
    "class ParallelAttentionBlock(nn.Module):\n",
    "    def __init__(self, channels, ratio=4):\n",
    "        super(ParallelAttentionBlock, self).__init__()\n",
    "        self.channel_attention = ChannelAttention(channels, ratio)\n",
    "        self.spatial_attention = SpatialAttention()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.channel_attention(x)\n",
    "        x2 = self.spatial_attention(x)\n",
    "        return x1 + x2\n",
    "\n",
    "class ConvLSTMBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(ConvLSTMBlock, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.bn = nn.BatchNorm1d(out_channels)\n",
    "        self.lstm = nn.LSTM(out_channels, out_channels, batch_first=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, channels, sequence_length)\n",
    "        x = F.relu(self.conv(x))\n",
    "        x = self.bn(x)\n",
    "        \n",
    "        # LSTM需要 (batch_size, sequence_length, features)\n",
    "        x = x.transpose(1, 2)  # (B, L, C)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x.transpose(1, 2)  # (B, C, L)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class ImprovedPhaseNetLSTMParallelAttentionCBAM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImprovedPhaseNetLSTMParallelAttentionCBAM, self).__init__()\n",
    "        \n",
    "        # 编码器部分\n",
    "        # 第一层卷积\n",
    "        self.conv1_1 = nn.Conv1d(3, 8, kernel_size=3, padding=1)\n",
    "        self.bn1_1 = nn.BatchNorm1d(8)\n",
    "        self.conv1_2 = nn.Conv1d(8, 8, kernel_size=3, padding=1)\n",
    "        self.bn1_2 = nn.BatchNorm1d(8)\n",
    "        self.lstm1 = nn.LSTM(8, 8, batch_first=True)\n",
    "        \n",
    "        # 下采样1\n",
    "        self.conv2_1 = nn.Conv1d(8, 8, kernel_size=3, stride=4, padding=1)\n",
    "        self.bn2_1 = nn.BatchNorm1d(8)\n",
    "        self.conv2_2 = nn.Conv1d(8, 11, kernel_size=3, padding=1)\n",
    "        self.bn2_2 = nn.BatchNorm1d(11)\n",
    "        self.lstm2 = nn.LSTM(11, 11, batch_first=True)\n",
    "        \n",
    "        # 下采样2\n",
    "        self.conv3_1 = nn.Conv1d(11, 11, kernel_size=3, stride=4, padding=1)\n",
    "        self.bn3_1 = nn.BatchNorm1d(11)\n",
    "        self.conv3_2 = nn.Conv1d(11, 16, kernel_size=3, padding=1)\n",
    "        self.bn3_2 = nn.BatchNorm1d(16)\n",
    "        self.lstm3 = nn.LSTM(16, 16, batch_first=True)\n",
    "        \n",
    "        # 下采样3\n",
    "        self.conv4_1 = nn.Conv1d(16, 16, kernel_size=3, stride=4, padding=1)\n",
    "        self.bn4_1 = nn.BatchNorm1d(16)\n",
    "        self.conv4_2 = nn.Conv1d(16, 22, kernel_size=3, padding=1)\n",
    "        self.bn4_2 = nn.BatchNorm1d(22)\n",
    "        self.lstm4 = nn.LSTM(22, 22, batch_first=True)\n",
    "        \n",
    "        # 下采样4\n",
    "        self.conv5_1 = nn.Conv1d(22, 22, kernel_size=3, stride=4, padding=1)\n",
    "        self.bn5_1 = nn.BatchNorm1d(22)\n",
    "        \n",
    "        # 解码器部分\n",
    "        # 上采样1\n",
    "        self.conv6 = nn.Conv1d(22, 32, kernel_size=3, padding=1)\n",
    "        self.bn6 = nn.BatchNorm1d(32)\n",
    "        self.deconv6 = nn.ConvTranspose1d(32, 22, kernel_size=3, stride=4, padding=1, output_padding=1)\n",
    "        self.bn6_up = nn.BatchNorm1d(22)\n",
    "        \n",
    "        # 上采样2\n",
    "        self.parallel_att4 = ParallelAttentionBlock(22)\n",
    "        self.conv7 = nn.Conv1d(44, 22, kernel_size=3, padding=1)  # 22 + 22 = 44\n",
    "        self.bn7 = nn.BatchNorm1d(22)\n",
    "        self.deconv7 = nn.ConvTranspose1d(22, 16, kernel_size=3, stride=4, padding=1, output_padding=2)\n",
    "        self.bn7_up = nn.BatchNorm1d(16)\n",
    "        self.cbam7 = CBAMBlock(16)\n",
    "        \n",
    "        # 上采样3\n",
    "        self.parallel_att3 = ParallelAttentionBlock(16)\n",
    "        self.conv8 = nn.Conv1d(32, 16, kernel_size=3, padding=1)  # 16 + 16 = 32\n",
    "        self.bn8 = nn.BatchNorm1d(16)\n",
    "        self.deconv8 = nn.ConvTranspose1d(16, 11, kernel_size=3, stride=4, padding=1, output_padding=3)\n",
    "        self.bn8_up = nn.BatchNorm1d(11)\n",
    "        self.cbam8 = CBAMBlock(11)\n",
    "        \n",
    "        # 上采样4\n",
    "        self.parallel_att2 = ParallelAttentionBlock(11)\n",
    "        self.conv9 = nn.Conv1d(22, 11, kernel_size=3, padding=1)  # 11 + 11 = 22\n",
    "        self.bn9 = nn.BatchNorm1d(11)\n",
    "        self.deconv9 = nn.ConvTranspose1d(11, 8, kernel_size=3, stride=4, padding=1, output_padding=3)\n",
    "        self.bn9_up = nn.BatchNorm1d(8)\n",
    "        self.cbam9 = CBAMBlock(8)\n",
    "        \n",
    "        # 最终输出\n",
    "        self.parallel_att1 = ParallelAttentionBlock(8)\n",
    "        self.conv10 = nn.Conv1d(16, 8, kernel_size=3, padding=1)  # 8 + 8 = 16\n",
    "        self.bn10 = nn.BatchNorm1d(8)\n",
    "        self.lstm_final = nn.LSTM(8, 3, batch_first=True)\n",
    "        self.conv_final = nn.Conv1d(3, 3, kernel_size=3, padding=1)\n",
    "        self.bn_final = nn.BatchNorm1d(3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 输入 x shape: (batch_size, sequence_length, channels)\n",
    "        # 转换为 (batch_size, channels, sequence_length)\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        # 编码器\n",
    "        # 第一层卷积\n",
    "        x = F.relu(self.conv1_1(x))\n",
    "        x = self.bn1_1(x)\n",
    "        x1 = F.relu(self.conv1_2(x))\n",
    "        x1 = self.bn1_2(x1)\n",
    "        \n",
    "        # LSTM\n",
    "        x1_lstm = x1.transpose(1, 2)  # (B, L, C)\n",
    "        x1_lstm, _ = self.lstm1(x1_lstm)\n",
    "        x1 = x1_lstm.transpose(1, 2)  # (B, C, L)\n",
    "        \n",
    "        # 下采样1\n",
    "        x2 = F.relu(self.conv2_1(x1))\n",
    "        x2 = self.bn2_1(x2)\n",
    "        x2 = F.relu(self.conv2_2(x2))\n",
    "        x2 = self.bn2_2(x2)\n",
    "        \n",
    "        x2_lstm = x2.transpose(1, 2)\n",
    "        x2_lstm, _ = self.lstm2(x2_lstm)\n",
    "        x2 = x2_lstm.transpose(1, 2)\n",
    "        \n",
    "        # 下采样2\n",
    "        x3 = F.relu(self.conv3_1(x2))\n",
    "        x3 = self.bn3_1(x3)\n",
    "        x3 = F.relu(self.conv3_2(x3))\n",
    "        x3 = self.bn3_2(x3)\n",
    "        \n",
    "        x3_lstm = x3.transpose(1, 2)\n",
    "        x3_lstm, _ = self.lstm3(x3_lstm)\n",
    "        x3 = x3_lstm.transpose(1, 2)\n",
    "        \n",
    "        # 下采样3\n",
    "        x4 = F.relu(self.conv4_1(x3))\n",
    "        x4 = self.bn4_1(x4)\n",
    "        x4 = F.relu(self.conv4_2(x4))\n",
    "        x4 = self.bn4_2(x4)\n",
    "        \n",
    "        x4_lstm = x4.transpose(1, 2)\n",
    "        x4_lstm, _ = self.lstm4(x4_lstm)\n",
    "        x4 = x4_lstm.transpose(1, 2)\n",
    "        \n",
    "        # 下采样4\n",
    "        x5 = F.relu(self.conv5_1(x4))\n",
    "        x5 = self.bn5_1(x5)\n",
    "        \n",
    "        # 解码器\n",
    "        # 上采样1\n",
    "        x6 = F.relu(self.conv6(x5))\n",
    "        x6 = self.bn6(x6)\n",
    "        x6 = self.deconv6(x6)\n",
    "        x6 = self.bn6_up(x6)\n",
    "        \n",
    "        # 上采样2\n",
    "        x4_att = self.parallel_att4(x4)\n",
    "        x7 = torch.cat([x6, x4_att], dim=1)\n",
    "        x7 = F.relu(self.conv7(x7))\n",
    "        x7 = self.bn7(x7)\n",
    "        x7 = self.deconv7(x7)\n",
    "        x7 = self.bn7_up(x7)\n",
    "        x7 = self.cbam7(x7)\n",
    "        \n",
    "        # 上采样3\n",
    "        x3_att = self.parallel_att3(x3)\n",
    "        x8 = torch.cat([x3_att, x7], dim=1)\n",
    "        x8 = F.relu(self.conv8(x8))\n",
    "        x8 = self.bn8(x8)\n",
    "        x8 = self.deconv8(x8)\n",
    "        x8 = self.bn8_up(x8)\n",
    "        x8 = self.cbam8(x8)\n",
    "        \n",
    "        # 上采样4\n",
    "        x2_att = self.parallel_att2(x2)\n",
    "        x9 = torch.cat([x2_att, x8], dim=1)\n",
    "        x9 = F.relu(self.conv9(x9))\n",
    "        x9 = self.bn9(x9)\n",
    "        x9 = self.deconv9(x9)\n",
    "        x9 = self.bn9_up(x9)\n",
    "        x9 = self.cbam9(x9)\n",
    "        \n",
    "        # 最终输出\n",
    "        x1_att = self.parallel_att1(x1)\n",
    "        x10 = torch.cat([x1_att, x9], dim=1)\n",
    "        x10 = F.relu(self.conv10(x10))\n",
    "        x10 = self.bn10(x10)\n",
    "        \n",
    "        # 最终LSTM\n",
    "        x10_lstm = x10.transpose(1, 2)  # (B, L, C)\n",
    "        x10_lstm, _ = self.lstm_final(x10_lstm)\n",
    "        x10 = x10_lstm.transpose(1, 2)  # (B, C, L)\n",
    "        \n",
    "        # 最终卷积层\n",
    "        x10 = self.conv_final(x10)\n",
    "        x10 = self.bn_final(x10)\n",
    "        \n",
    "        # 应用softmax\n",
    "        #x10 = F.softmax(x10, dim=1)\n",
    "        \n",
    "        # 转换回 (batch_size, sequence_length, channels)\n",
    "        output = x10.transpose(1, 2)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# 创建模型实例\n",
    "def create_improved_phasenet():\n",
    "    return ImprovedPhaseNetLSTMParallelAttentionCBAM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8817ed10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GaoYuan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
