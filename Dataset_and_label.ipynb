{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c75cf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader,Subset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a5ec0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_phasenet_label(p_arrival, s_arrival, length=6000):\n",
    "    \"\"\"\n",
    "    构建修改后的PhaseNet标签\n",
    "    \n",
    "    Args:\n",
    "        p_arrival: P波到达时间点（样本索引），如果为None表示无P波\n",
    "        s_arrival: S波到达时间点（样本索引），如果为None表示无S波  \n",
    "        length: 波形长度\n",
    "    \n",
    "    Returns:\n",
    "        label: shape为(length,)的标签数组\n",
    "        - 0: 背景噪声\n",
    "        - 1: P_arrival到S_arrival之间\n",
    "        - 2: S_arrival到P+3×(S-P)之间\n",
    "    \"\"\"\n",
    "    # 初始化标签数组，默认全为0（背景噪声）\n",
    "    label = np.zeros(length, dtype=np.int64)\n",
    "    \n",
    "    # 检查P波和S波是否都存在且有效\n",
    "    if (p_arrival is not None and s_arrival is not None and \n",
    "        0 <= p_arrival < length and 0 <= s_arrival < length and \n",
    "        p_arrival < s_arrival):  # 确保P波在S波之前\n",
    "        \n",
    "        # 计算各个区间的边界\n",
    "        p_start = int(p_arrival)\n",
    "        s_start = int(s_arrival)\n",
    "        \n",
    "        # 计算第二个区间的结束点: P + 3×(S-P) = 3S - 2P\n",
    "        s_p_diff = s_start - p_start  # S-P时间差\n",
    "        region2_end = s_start + 3 * s_p_diff  # S + 3×(S-P)\n",
    "        region2_end = min(region2_end, length - 1)  # 确保不超出数组边界\n",
    "        \n",
    "        # 设置标签\n",
    "        # 区间1: P_arrival到S_arrival之间设为1\n",
    "        label[p_start:s_start + 1] = 1\n",
    "        \n",
    "        # 区间2: S_arrival到P+3×(S-P)之间设为2\n",
    "        if region2_end > s_start:\n",
    "            label[s_start:region2_end + 1] = 2\n",
    "    \n",
    "    return torch.from_numpy(label).long()  # 使用long类型适合分类标签\n",
    "\n",
    "\n",
    "# 如果您需要one-hot编码版本（用于某些损失函数）\n",
    "def build_phasenet_label_onehot(p_arrival, s_arrival, length=6000):\n",
    "    \"\"\"\n",
    "    构建one-hot编码版本的标签\n",
    "    \n",
    "    Returns:\n",
    "        label: shape为(length, 3)的one-hot标签数组\n",
    "        通道0 噪声\n",
    "        通道1 P\n",
    "        通道2 S\n",
    "    \"\"\"\n",
    "    # 获取类别标签\n",
    "    class_labels = build_phasenet_label(p_arrival, s_arrival, length)\n",
    "    \n",
    "    # 转换为one-hot编码\n",
    "    label_onehot = np.zeros((length, 3), dtype=np.float32)\n",
    "    for i in range(3):\n",
    "        label_onehot[:, i] = (class_labels == i).float()\n",
    "    \n",
    "    return torch.from_numpy(label_onehot).float()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb172748",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SteadDataset(Dataset):\n",
    "    def __init__(self, npy_dir, csv_path):\n",
    "        self.npy_dir = npy_dir\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.trace_names = self.df['trace_name'].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.trace_names)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        trace_name = self.trace_names[idx]\n",
    "        npy_path = os.path.join(self.npy_dir, f\"{trace_name}.npy\")\n",
    "        data = np.load(npy_path)\n",
    "        data_tensor = torch.from_numpy(data).float()\n",
    "\n",
    "\n",
    "        # 构建标签\n",
    "        row = self.df.iloc[idx]\n",
    "        if row['trace_category'] == 'noise':\n",
    "            p_label, s_label = None, None\n",
    "        else:\n",
    "            p_label = row['p_arrival_sample']\n",
    "            s_label = row['s_arrival_sample']\n",
    "        label_tensor = build_phasenet_label_onehot(p_label, s_label, length=data.shape[0])\n",
    "\n",
    "\n",
    "        # Z-score 标准化\n",
    "        mean = data_tensor.mean(dim=0, keepdim=True)\n",
    "        std = data_tensor.std(dim=0, keepdim=True)\n",
    "        eps = 1e-8\n",
    "        data_tensor = (data_tensor - mean) / (std + eps)\n",
    "\n",
    "        return data_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d9ac99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据路径\n",
    "h5_path = \"D:\\\\merge.hdf5\"\n",
    "npy_path = 'D:\\\\STEAD_npy'\n",
    "train_path = \"E:\\\\STEAD_dataset\\\\manual_train.csv\"\n",
    "val_path = \"E:\\\\STEAD_dataset\\\\manual_val.csv\"\n",
    "test_path = \"E:\\\\STEAD_dataset\\\\manual_test.csv\"\n",
    "\n",
    "# 创建完整数据集\n",
    "print(\"Loading full datasets...\")\n",
    "\n",
    "# 创建训练集（启用数据增强）\n",
    "train_dataset_full = SteadDataset(\n",
    "    npy_dir=npy_path,\n",
    "    csv_path=train_path,\n",
    "    #augment=True,\n",
    "    #augment_params=augment_params\n",
    ")\n",
    "# 创建验证集（不使用数据增强）\n",
    "val_dataset_full = SteadDataset(\n",
    "    npy_dir=npy_path,\n",
    "    csv_path=val_path,\n",
    "    augment=False\n",
    ")\n",
    "# 创建测试集（不使用数据增强）\n",
    "test_dataset_full = SteadDataset(\n",
    "    npy_dir=npy_path,\n",
    "    csv_path=test_path,\n",
    "    augment=False\n",
    ")\n",
    "\n",
    "# 计算前10%的样本数量\n",
    "train_size = len(train_dataset_full)\n",
    "val_size = len(val_dataset_full)\n",
    "test_size = len(test_dataset_full)\n",
    "\n",
    "train_subset_size = math.ceil(train_size * 0.1)  # 向上取整确保至少有一些样本\n",
    "val_subset_size = math.ceil(val_size * 0.1)\n",
    "test_subset_size = math.ceil(test_size * 0.1)\n",
    "\n",
    "print(f\"Original dataset sizes:\")\n",
    "print(f\"  Train: {train_size} -> Using: {train_subset_size} (10%)\")\n",
    "print(f\"  Val: {val_size} -> Using: {val_subset_size} (10%)\")\n",
    "print(f\"  Test: {test_size} -> Using: {test_subset_size} (10%)\")\n",
    "\n",
    "# 创建前10%的子集\n",
    "train_indices = list(range(train_subset_size))\n",
    "val_indices = list(range(val_subset_size))\n",
    "test_indices = list(range(test_subset_size))\n",
    "\n",
    "train_dataset = Subset(train_dataset_full, train_indices)\n",
    "val_dataset = Subset(val_dataset_full, val_indices)\n",
    "test_dataset = Subset(test_dataset_full, test_indices)\n",
    "\n",
    "# 创建数据加载器\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=128, \n",
    "    shuffle=True, \n",
    "    #num_workers=8,\n",
    "    pin_memory=True,\n",
    "    #prefetch_factor=2\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=128, \n",
    "    shuffle=False, \n",
    "    #num_workers=8,\n",
    "    pin_memory=True,\n",
    "    #prefetch_factor=2\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=128, \n",
    "    shuffle=False, \n",
    "    #num_workers=8,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\nFinal dataset sizes:\")\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Val samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8817ed10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GaoYuan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
